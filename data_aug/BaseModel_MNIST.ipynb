{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "r4e_U5zYaoYa"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "l5ObFPzTbdaJ"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edda9f1f41864ec98bef2609bd85d0f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a57a47daf7493d8483497ed57eb64e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a710651d94d4230a285bde475bfbcc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceac117bf80d4a5e9fcc18565286a8bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "validation_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(dataset=training_dataset, batch_size=100, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=100, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "5w2eR5IAbMHS"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self) :\n",
    "        super(Net, self).__init__()\n",
    "        self.conv_layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5, padding=2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.pool_layer1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv_layer2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, kernel_size=5),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.pool_layer2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.C5_layer = nn.Sequential(\n",
    "            nn.Linear(5*5*16, 120),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_layer1 = nn.Sequential(\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_layer2 = nn.Linear(84, 10)\n",
    "        \n",
    "    \n",
    "    def forward(self, x) :\n",
    "        output = self.conv_layer1(x)\n",
    "        output = self.pool_layer1(output)\n",
    "        output = self.conv_layer2(output)\n",
    "        output = self.pool_layer2(output)\n",
    "        output = output.reshape(-1,5*5*16)\n",
    "        output = self.C5_layer(output)\n",
    "        output = self.fc_layer1(output)\n",
    "        output = self.fc_layer2(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "3VjPA5Hfa1dp"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dfpcyz8tbDX5",
    "outputId": "c26f5784-4d55-425f-b02b-aee59f299d8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "epoch:  1\n",
      "training loss: 0.35940, acc: 88.886665\n",
      "validation loss: 0.09442, acc: 97.029999\n",
      "===================================================\n",
      "epoch:  2\n",
      "training loss: 0.09398, acc: 97.110001\n",
      "validation loss: 0.06281, acc: 98.110001\n",
      "===================================================\n",
      "epoch:  3\n",
      "training loss: 0.06575, acc: 97.956673\n",
      "validation loss: 0.05854, acc: 98.159996\n",
      "===================================================\n",
      "epoch:  4\n",
      "training loss: 0.05015, acc: 98.416672\n",
      "validation loss: 0.04705, acc: 98.610001\n",
      "===================================================\n",
      "epoch:  5\n",
      "training loss: 0.04199, acc: 98.690002\n",
      "validation loss: 0.04450, acc: 98.579994\n",
      "===================================================\n",
      "epoch:  6\n",
      "training loss: 0.03365, acc: 98.955002\n",
      "validation loss: 0.03220, acc: 99.019997\n",
      "===================================================\n",
      "epoch:  7\n",
      "training loss: 0.02947, acc: 99.046669\n",
      "validation loss: 0.02761, acc: 99.150002\n",
      "===================================================\n",
      "epoch:  8\n",
      "training loss: 0.02546, acc: 99.185005\n",
      "validation loss: 0.04078, acc: 98.629997\n",
      "===================================================\n",
      "epoch:  9\n",
      "training loss: 0.02262, acc: 99.236671\n",
      "validation loss: 0.03715, acc: 98.849998\n",
      "===================================================\n",
      "epoch:  10\n",
      "training loss: 0.01955, acc: 99.345001\n",
      "validation loss: 0.03495, acc: 98.970001\n",
      "===================================================\n",
      "epoch:  11\n",
      "training loss: 0.01639, acc: 99.471672\n",
      "validation loss: 0.04134, acc: 98.619995\n",
      "===================================================\n",
      "epoch:  12\n",
      "training loss: 0.01513, acc: 99.490005\n",
      "validation loss: 0.03121, acc: 98.949997\n"
     ]
    }
   ],
   "source": [
    "epochs = 12\n",
    "running_loss_history = []\n",
    "running_correct_history = []\n",
    "validation_running_loss_history = []\n",
    "validation_running_correct_history = []\n",
    "\n",
    "for e in range(epochs):\n",
    "\n",
    "  running_loss = 0.0\n",
    "  running_correct = 0.0\n",
    "  validation_running_loss = 0.0\n",
    "  validation_running_correct = 0.0\n",
    "\n",
    "  for inputs, labels in training_loader:\n",
    "\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "    running_correct += torch.sum(preds == labels.data)\n",
    "    running_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "  else:\n",
    "    # 훈련팔 필요가 없으므로 메모리 절약\n",
    "    with torch.no_grad():\n",
    "\n",
    "      for val_input, val_label in validation_loader:\n",
    "\n",
    "        val_input = val_input.to(device)\n",
    "        val_label = val_label.to(device)\n",
    "        val_outputs = model(val_input)\n",
    "        val_loss = criterion(val_outputs, val_label)\n",
    "\n",
    "        _, val_preds = torch.max(val_outputs, 1)\n",
    "        validation_running_loss += val_loss.item()\n",
    "        validation_running_correct += torch.sum(val_preds == val_label.data)\n",
    "\n",
    "\n",
    "    epoch_loss = running_loss / len(training_loader)\n",
    "    epoch_acc = running_correct.float() / len(training_loader)\n",
    "    running_loss_history.append(epoch_loss)\n",
    "    running_correct_history.append(epoch_acc)\n",
    "\n",
    "    val_epoch_loss = validation_running_loss / len(validation_loader)\n",
    "    val_epoch_acc = validation_running_correct.float() / len(validation_loader)\n",
    "    validation_running_loss_history.append(val_epoch_loss)\n",
    "    validation_running_correct_history.append(val_epoch_acc)\n",
    "\n",
    "    print(\"===================================================\")\n",
    "    print(\"epoch: \", e + 1)\n",
    "    print(\"training loss: {:.5f}, acc: {:5f}\".format(epoch_loss, epoch_acc))\n",
    "    print(\"validation loss: {:.5f}, acc: {:5f}\".format(val_epoch_loss, val_epoch_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'MNIST_BaseModel_weights.pth')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "951cb00ebd66971857c5880c838618f69a707a5e4d3bee9fba08c4850d7bcc87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
